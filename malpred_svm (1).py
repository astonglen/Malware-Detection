from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import BinaryClassificationEvaluator,RegressionEvaluator
from pyspark.ml.classification import LinearSVC
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
import time
import matplotlib.pyplot as plt
from pyspark.sql.functions import expr
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

if __name__ == "__main__":
    spark = SparkSession.builder \
        .appName("Malware Prediction SVM") \
        .config("spark.executor.memory", '12g') \
        .config("spark.driver.maxResultSize", '12g') \
        .config("spark.kryoserializer.buffer.max", '512m') \
        .getOrCreate()

    # Read the Parquet file into a DataFrame
    reduced_features_with_target_sdf = spark.read.parquet("gs://project-bucketgcp-a/EDAed_data.parquet")


   # List of subsample percentages
subsamples = [1.0, 0.8, 0.6, 0.4, 0.2]
target_column="HasDetections"
# Initialize evaluators
# BinaryClassificationEvaluator for computing Area Under ROC Curve (AUC)
binary_evaluator = BinaryClassificationEvaluator(labelCol=target_column)

# RegressionEvaluator for computing Root Mean Square Error (RMSE)
regression_evaluator = RegressionEvaluator(labelCol=target_column, metricName="rmse")

# MulticlassClassificationEvaluator for computing precision, recall, and F1 score
multi_evaluator = MulticlassClassificationEvaluator(labelCol=target_column)

timing_values = []  # List to store timing values
subsample_percentages = []  # List to store subsample percentages
rmse_values = []  # List to store RMSE values
# Loop over each subsample
for subsample in subsamples:
    print(f"Training with {subsample*100}% data...")

    start_time = time.time()  # Start the timer

    # Sample data
    # False indicates that we are not sampling with replacement
    sample_data = reduced_features_with_target_sdf.sample(False, float(subsample), seed=42)


    # Split the data into training set (80%) and test set (20%)
    train_data, test_data = sample_data.randomSplit([0.8, 0.2], seed=42)

    # Define the SVM model
    svm = LinearSVC(featuresCol='features', labelCol=target_column)

    # Set up the parameter grid
    # The cross-validator will try all combinations of these parameters
    paramGrid = ParamGridBuilder() \
        .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \
        .addGrid(svm.fitIntercept, [False, True]) \
        .addGrid(svm.maxIter, [10, 100, 1000]) \
        .build()

    # Set up the cross-validator
    # estimator is the machine learning algorithm to be used (SVM in this case)
    # estimatorParamMaps is the grid of parameters to try
    # evaluator is the metric used to measure the performance of the models
    # numFolds is the number of cross-validation folds
    crossval = CrossValidator(estimator=svm,
                            estimatorParamMaps=paramGrid,
                            evaluator=BinaryClassificationEvaluator(labelCol=target_column),
                            numFolds=3)

    # Train the model
    cv_model = crossval.fit(train_data)

    # Make predictions on the test set
    predictions = cv_model.transform(test_data)

    # Evaluate the model using AUC and RMSE
    auc = binary_evaluator.evaluate(predictions)
    rmse = regression_evaluator.evaluate(predictions)
    # Compute accuracy manually
    total_count = predictions.count()
    correct_count = predictions.filter(expr('prediction == HasDetections')).count()
    accuracy = correct_count / total_count
    precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "weightedPrecision"})
    recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "weightedRecall"})
    f1_score = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: "f1"})

    end_time = time.time()  # Stop the timer
    # Get the number of cores used
    spark_context = spark.sparkContext
    num_cores = spark_context.defaultParallelism
    # Get the Spark application ID
    app_id = spark_context.applicationId

    timing_values.append(end_time - start_time)
    subsample_percentages.append(subsample * 100)
    rmse_values.append(rmse)

    print(f"Subsample: {subsample*100}%")
    print(f"AUC: {auc}")
    print(f"RMSE: {rmse}")
    print(f"Accuracy: {accuracy}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1_score}")
    print(f"Time taken: {end_time - start_time} seconds\n")
    print(f"Number of cores used: {num_cores}")    
    print(f"Spark application ID: {app_id}")
    
print("Scaling Effect on Time:")
plt.plot(subsample_percentages, timing_values)
plt.xlabel("Subsample Percentage")
plt.ylabel("Time (seconds)")
plt.title("Time vs. Percentage of Data Used")
plt.show()

print("Scaling Effect on RMSE:")
plt.plot(subsample_percentages, rmse_values)
plt.xlabel("Subsample Percentage")
plt.ylabel("RMSE")
plt.title("RMSE vs. Percentage of Data Used")
plt.show()

 