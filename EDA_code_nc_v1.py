
from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import udf, when, col, mean
from pyspark.sql.types import StringType
from pyspark.sql.functions import countDistinct, col
from pyspark.sql.functions import sum
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler
from pyspark.ml import Pipeline
from pyspark.ml.feature import PCA
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
import time
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA, StringIndexer, OneHotEncoder
import pandas as pd
import numpy as np
from pyspark.ml.stat import Correlation
from sklearn.random_projection import GaussianRandomProjection
from pyspark.sql.functions import monotonically_increasing_id
from pyspark.sql import Row
from pyspark.sql.types import DoubleType, IntegerType
import timeit
from pyspark.sql.functions import col, when, udf
from pyspark.sql.types import StringType, IntegerType, DoubleType, FloatType, BooleanType, ByteType, ShortType
from pyspark.ml import Pipeline


if __name__ == "__main__":
    spark5 = SparkSession.builder \
    .appName("Malware Prediction EDA v17") \
    .config("spark.executor.memory", '45g') \
    .config("spark.driver.maxResultSize", '45g') \
    .config("spark.kryoserializer.buffer.max", '512m') \
    .getOrCreate()
    # Define the schema
    schema = StructType([
        StructField('MachineIdentifier', StringType(), True),
        StructField('ProductName', StringType(), True),
        StructField('EngineVersion', StringType(), True),
        StructField('AppVersion', StringType(), True),
        StructField('AvSigVersion', StringType(), True),
        StructField('IsBeta', ByteType(), True),
        StructField('RtpStateBitfield', FloatType(), True),
        StructField('IsSxsPassiveMode', ByteType(), True),
        StructField('DefaultBrowsersIdentifier', FloatType(), True),
        StructField('AVProductStatesIdentifier', FloatType(), True),
        StructField('AVProductsInstalled', FloatType(), True),
        StructField('AVProductsEnabled', FloatType(), True),
        StructField('HasTpm', ByteType(), True),
        StructField('CountryIdentifier', ShortType(), True),
        StructField('CityIdentifier', FloatType(), True),
        StructField('OrganizationIdentifier', FloatType(), True),
        StructField('GeoNameIdentifier', FloatType(), True),
        StructField('LocaleEnglishNameIdentifier', ByteType(), True),
        StructField('Platform', StringType(), True),
        StructField('Processor', StringType(), True),
        StructField('OsVer', StringType(), True),
        StructField('OsBuild', ShortType(), True),
        StructField('OsSuite', ShortType(), True),
        StructField('OsPlatformSubRelease', StringType(), True),
        StructField('OsBuildLab', StringType(), True),
        StructField('SkuEdition', StringType(), True),
        StructField('IsProtected', FloatType(), True),
        StructField('AutoSampleOptIn', ByteType(), True),
        StructField('PuaMode', StringType(), True),
        StructField('SMode', FloatType(), True),
        StructField('IeVerIdentifier', FloatType(), True),
        StructField('SmartScreen', StringType(), True),
        StructField('Firewall', FloatType(), True),
        StructField('UacLuaenable', FloatType(), True),
        StructField('Census_MDC2FormFactor', StringType(), True),
        StructField('Census_DeviceFamily', StringType(), True),
        StructField('Census_OEMNameIdentifier', FloatType(), True),
        StructField('Census_OEMModelIdentifier', FloatType(), True),
        StructField('Census_ProcessorCoreCount', FloatType(), True),
        StructField('Census_ProcessorManufacturerIdentifier', FloatType(), True),
        StructField('Census_ProcessorModelIdentifier', FloatType(), True),
        StructField('Census_ProcessorClass', StringType(), True),
        StructField('Census_PrimaryDiskTotalCapacity', FloatType(), True),
        StructField('Census_PrimaryDiskTypeName', StringType(), True),
        StructField('Census_SystemVolumeTotalCapacity', FloatType(), True),
        StructField('Census_HasOpticalDiskDrive', ByteType(), True),
        StructField('Census_TotalPhysicalRAM', FloatType(), True),
        StructField('Census_ChassisTypeName', StringType(), True),
        StructField('Census_InternalPrimaryDiagonalDisplaySizeInInches', FloatType(), True),
        StructField('Census_InternalPrimaryDisplayResolutionHorizontal', FloatType(), True),
        StructField('Census_InternalPrimaryDisplayResolutionVertical', FloatType(), True),
        StructField('Census_PowerPlatformRoleName', StringType(), True),
        StructField('Census_InternalBatteryType', StringType(), True),
        StructField('Census_InternalBatteryNumberOfCharges', FloatType(), True),
        StructField('Census_OSVersion', StringType(), True),
        StructField('Census_OSArchitecture', StringType(), True),
        StructField('Census_OSBranch', StringType(), True),
        StructField('Census_OSBuildNumber', ShortType(), True),
        StructField('Census_OSBuildRevision', IntegerType(), True),
        StructField('Census_OSEdition', StringType(), True),
        StructField('Census_OSSkuName', StringType(), True),
        StructField('Census_OSInstallTypeName', StringType(), True),
        StructField('Census_OSInstallLanguageIdentifier', FloatType(), True),
        StructField('Census_OSUILocaleIdentifier', ShortType(), True),
        StructField('Census_OSWUAutoUpdateOptionsName', StringType(), True),
        StructField('Census_IsPortableOperatingSystem', ByteType(), True),
        StructField('Census_GenuineStateName', StringType(), True),
        StructField('Census_ActivationChannel', StringType(), True),
        StructField('Census_IsFlightingInternal', FloatType(), True),
        StructField('Census_IsFlightsDisabled', FloatType(), True),
        StructField('Census_FlightRing', StringType(), True),
        StructField('Census_ThresholdOptIn', FloatType(), True),
        StructField('Census_FirmwareManufacturerIdentifier', FloatType(), True),
        StructField('Census_FirmwareVersionIdentifier', FloatType(), True),
        StructField('Census_IsSecureBootEnabled', ByteType(), True),
        StructField('Census_IsWIMBootEnabled', FloatType(), True),
        StructField('Census_IsVirtualDevice', FloatType(), True),
        StructField('Census_IsTouchEnabled', ByteType(), True),
        StructField('Census_IsPenCapable', ByteType(), True),
        StructField('Census_IsAlwaysOnAlwaysConnectedCapable', FloatType(), True),
        StructField('Wdft_IsGamer', FloatType(), True),
        StructField('Wdft_RegionIdentifier', FloatType(), True),
        StructField('HasDetections', ByteType(), True)
    ])
    # Record the start time
    start_time = timeit.default_timer()
    column_names = [  "AVProductStatesIdentifier", "IsProtected", "Census_ProcessorCoreCount", "Census_IsWIMBootEnabled",
                    "RtpStateBitfield", "Census_InternalPrimaryDiagonalDisplaySizeInInches", "Wdft_RegionIdentifier",
                    "Census_InternalPrimaryDisplayResolutionHorizontal", "Census_OSBuildNumber",  "Firewall",
                    "OsBuild", "Census_ProcessorModelIdentifier","Census_IsVirtualDevice","Wdft_IsGamer",
                                          "SmartScreen", "Census_OSSkuName", "Census_OSEdition", 
                                            "Census_ActivationChannel", "Census_ChassisTypeName", 
                                              "Census_PowerPlatformRoleName", "Census_FlightRing",
                                                  "SkuEdition", "Census_OSWUAutoUpdateOptionsName", 
                                                    "Census_PrimaryDiskTypeName", "Census_GenuineStateName"]
    
     # Load the entire dataset into a PySpark DataFrame
    df_full = spark5.read.csv("gs://bigdatamalpredv2/train.csv", header=True,schema=schema)

    # Select the required columns
    selected_columns = ['MachineIdentifier', 'HasDetections'] + column_names
    selected_data = df_full.select(*selected_columns)


    #Take 1.2M rows of data
    fraction = 0.3
    seed = 42  # Choose any seed value for reproducibility
    selected_data = selected_data.sample(False, fraction, seed)

        # Get the schema and identify column types
    numerical_columns = []
    binary_columns = []
    categorical_columns = []

    for field in selected_data.schema.fields:
        column_name = field.name
        column_dtype = field.dataType

        if isinstance(column_dtype, IntegerType) or isinstance(column_dtype, DoubleType) or isinstance(column_dtype,  FloatType) or isinstance(column_dtype, ByteType):
            # Numerical and binary columns
            numerical_columns.append(column_name)
            if set(selected_data.select(column_name).distinct().collect()) == {0, 1}:
                binary_columns.append(column_name)
        elif isinstance(column_dtype, StringType) or isinstance(column_dtype, BooleanType):
            # Categorical columns
            categorical_columns.append(column_name)

    print("Numerical columns:", numerical_columns)
    print("Binary columns:", binary_columns)
    print("Categorical columns:", categorical_columns)
    # Record the time taken for the previous steps
    print(f"Time taken for loading data and identifying column types: {timeit.default_timer() - start_time} seconds")

    df=selected_data

    missing_ratio_threshold = 0.7
    missing_count = df.select([(df[c].isNull().cast("integer")).alias(c) for c in df.columns]).groupBy().sum()
    total_rows = df.count()
    columns_to_remove = [c for c, count in zip(df.columns, missing_count.first()) if count / total_rows > missing_ratio_threshold]
    print('missing values treated')
    # Drop columns with a high percentage of missing values
    df = df.drop(*columns_to_remove)

    categorical_columns = [column for column in categorical_columns if column != "MachineIdentifier"]
    binary_columns = [column for column in binary_columns if column != "HasDetections"]
    numerical_columns = [column for column in numerical_columns if column != "HasDetections"]
    categorical_columns = [column for column in categorical_columns if column != "HasDetections"]


    # Count distinct values for each categorical feature
    for feature in categorical_columns:
        distinct_count = df.select(feature).distinct().count()
        print(f"Distinct count for {feature}: {distinct_count}")

    machine_identifier_column = df.select("MachineIdentifier")
    df = df.drop("MachineIdentifier")


    new_train_data = df

    print('data preprocessing')
    def replace_smartscreen(val):
        ''' cleaning category values to reduce number of categories for smartscreen feature '''
        val = val.lower()
        if val in 'block':
            return 'block'
        elif val in 'existsnotset':
            return 'existnotset'
        elif val in 'off':
            return 'off'
        elif val in 'prompt':
            return 'prompt'
        elif val in 'requireadmin':
            return 'requireadmin'
        elif val in 'warn':
            return 'warn'
        elif val in 'on':
            return 'on'
        elif 'null' in val:
            return 'unknown'
        else:
            return 'unknown'

    # Register the replace_smartscreen function as a UDF
    replace_smartscreen_udf = udf(replace_smartscreen, StringType())


        # Define the replace functions
    def replace_edition(val):
        if val is None:
            return 'unknown'
        val = val.lower()
        if 'cloud' in val:
            return val
        elif 'core' in val:
            return 'core'
        elif 'education' in val:
            return 'education'
        elif 'enterprise' in val:
            return 'enterprise'
        elif 'pro' in val:
            return 'pro'
        elif 'server' in val:
            return 'server'
        elif 'home' in val:
            return 'home'
        elif 'null' in val:
            return 'unknown'
        else:
            return 'unknown'

    def replace_channel(val):
        if val is None:
            return 'unknown'
        val = val.lower()
        if 'oem' in val:
            return 'oem'
        elif 'retail' in val:
            return 'retail'
        elif 'volume' in val:
            return 'volume'
        elif 'null' in val:
            return 'unknown'
        else:
            return 'unknown'

    def replace_skuname(val):
        if val is None:
            return 'unknown'
        val = val.lower()
        if 'cloud' in val:
            return val
        elif 'core' in val:
            return 'core'
        elif 'education' in val:
            return 'education'
        elif 'enterprise' in val:
            return 'enterprise'
        elif 'pro' in val:
            return 'pro'
        elif 'server' in val:
            return 'server'
        elif 'home' in val:
            return 'home'
        elif 'null' in val:
            return 'unknown'
        else:
            return 'unknown'





    # Define the replace functions as UDFs
    replace_channel_udf = udf(replace_channel, StringType())
    replace_edition_udf = udf(replace_edition, StringType())
    replace_skuname_udf = udf(replace_skuname, StringType())


    def fill_missing_values(data, features):
        numerical_features, binary_features, categorical_features = features

        # Replace null values with -1 in numerical_features
        for feature in numerical_features:
            data = data.withColumn(feature, when(col(feature).isNull(), -1).otherwise(col(feature)))

        # Replace null values with mode value of that feature in binary_features
        # Replace null values with mode value of that feature in binary_features
        for feature in binary_features:
            mode_value = data.groupBy(col(feature)).count().orderBy(col('count').desc()).collect()[0][feature]
            data = data.withColumn(feature, when(col(feature).isNull(), mode_value).otherwise(col(feature)))

        # Apply replace functions for specific columns
        data = data.withColumn('SmartScreen', replace_smartscreen_udf(col('SmartScreen')))
        data = data.withColumn('Census_OSSkuName', replace_skuname_udf(col('Census_OSSkuName')))
        data = data.withColumn('Census_OSEdition', replace_edition_udf(col('Census_OSEdition')))
        data = data.withColumn('Census_ActivationChannel', replace_channel_udf(col('Census_ActivationChannel')))

        # Replace specific values in certain columns
        columns_to_replace = {

            'Census_PowerPlatformRoleName': ['UNKNOWN', 'Unspecified'],
            'Census_FlightRing': ['Invalid', 'Unknown'],
            'Census_OSSkuName': ['UNDEFINED'],
            'SkuEdition': ['Invalid'],
            'Census_OSWUAutoUpdateOptionsName': ['UNKNOWN'],
            'Census_PrimaryDiskTypeName': ['UNKNOWN', 'Unspecified'],
            'Census_GenuineStateName': ['UNKNOWN']
        }

        for column, values_to_replace in columns_to_replace.items():
            for value_to_replace in values_to_replace:
                data = data.withColumn(column, when(col(column) == value_to_replace, 'unknown').otherwise(col(column)))

        # Replace null values with 'unknown' in categorical_features
        for feature in categorical_features:
            data = data.withColumn(feature, when(col(feature).isNull(), 'unknown').otherwise(col(feature)))

        return data


    numerical_features=numerical_columns.copy()
    binary_features=binary_columns.copy()
    categorical_features=categorical_columns.copy()


    features = (numerical_features,binary_features,categorical_features)
    updated_train_data = fill_missing_values(new_train_data, features)
    new_train_data = new_train_data.dropna()


    target_column = "HasDetections"



    valid_categorical_columns = []
    for col_name in categorical_columns:
        distinct_values_count = new_train_data.select(col_name).distinct().count()
        if distinct_values_count > 1:
            valid_categorical_columns.append(col_name)
    print('valid cates',valid_categorical_columns)
    # Perform one-hot encoding for each valid categorical column
    encoded_cols = []
    for col_name in valid_categorical_columns:
        indexer = StringIndexer(inputCol=col_name, outputCol=f"{col_name}_index")
        encoder = OneHotEncoder(inputCol=f"{col_name}_index", outputCol=f"{col_name}_encoded")
        new_train_data = indexer.fit(new_train_data).transform(new_train_data)
        new_train_data = encoder.fit(new_train_data).transform(new_train_data)
        encoded_cols.append(f"{col_name}_encoded")


    print('done')


         # Assemble the feature vectors
    start_time = time.time()
    print("Assembling feature vectors...")

    all_feature_columns = numerical_features + binary_features + encoded_cols
    print(all_feature_columns)
    assembler = VectorAssembler(inputCols=all_feature_columns, outputCol="features")
    new_train_data = assembler.transform(new_train_data)

    end_time = time.time()
    print(f"Feature vectors assembled. Time taken: {end_time - start_time} seconds")

    print("correlation code started....")


    assembler_numerical = VectorAssembler(inputCols=numerical_features, outputCol="numerical_features")
    assembled_numerical_data = assembler_numerical.transform(new_train_data)
    correlation_matrix = Correlation.corr(assembled_numerical_data, "numerical_features").head()[0]

    # Set a threshold for high correlation
    high_correlation_threshold = 0.9

    # Identify highly correlated variables and remove them
    highly_correlated_features = set()
    for i in range(correlation_matrix.numRows):
        for j in range(i+1, correlation_matrix.numCols):
            if abs(correlation_matrix[i, j]) > high_correlation_threshold:
                highly_correlated_features.add(numerical_features[i])
                highly_correlated_features.add(numerical_features[j])

    updated_train_data = new_train_data.drop(*highly_correlated_features)

    # If you want to update the feature vector after removing highly correlated features,
    # you can create a new feature vector using the updated columns.
    updated_feature_columns = [col for col in all_feature_columns if col not in highly_correlated_features]
    updated_assembler = VectorAssembler(inputCols=updated_feature_columns, outputCol="updated_features")
    new_train_data = updated_assembler.transform(updated_train_data)

    print("correlation code done")

    # Scale the feature vectors
    start_time = time.time()
    print("Scaling feature vectors...")

    scaler = StandardScaler(inputCol="features", outputCol="scaled_features", withStd=True, withMean=True)
    scaler_model = scaler.fit(new_train_data)
    new_train_data = scaler_model.transform(new_train_data)

    end_time = time.time()
    print(f"Feature vectors scaled. Time taken: {end_time - start_time} seconds")


    # Convert the PySpark DataFrame to a Pandas DataFrame
    pdf = new_train_data.select("scaled_features").toPandas()

    # Extract the scaled_features column as a NumPy array
    scaled_features_np = np.array(pdf["scaled_features"].tolist())

    # Set the number of components you'd like to retain
    num_components = 15

    # Perform Gaussian Random Projection
    start_time = time.time()
    print("Performing Gaussian Random Projection...")

    grp = GaussianRandomProjection(n_components=num_components)
    reduced_features = grp.fit_transform(scaled_features_np)

    end_time = time.time()
    print(f"Gaussian Random Projection completed. Time taken: {end_time - start_time} seconds")

    # Convert the NumPy array back to a PySpark DataFrame
    reduced_features_df = pd.DataFrame(reduced_features, columns=[f"component_{i}" for i in range(num_components)])
    reduced_features_sdf = spark5.createDataFrame(reduced_features_df)

    # Add an index column to the original DataFrame
    indexed_target_sdf = new_train_data.select(target_column).rdd.zipWithIndex().map(lambda x: Row(**{target_column: x[0][target_column], 'index': x[1]})).toDF()

    # Add an index column to the reduced_features_sdf
    indexed_reduced_features_sdf = reduced_features_sdf.rdd.zipWithIndex().map(lambda x: Row(**{f"component_{i}": x[0][f"component_{i}"] for i in range(num_components)}, index=x[1])).toDF()

    # Join the reduced features with the target column
    reduced_features_with_target_sdf = indexed_reduced_features_sdf.join(indexed_target_sdf, "index").drop("index")

   # Assemble the components into a single 'features' column
    component_columns = [f"component_{i}" for i in range(num_components)]
    assembler = VectorAssembler(inputCols=component_columns, outputCol="features")
    reduced_features_with_target_sdf = assembler.transform(reduced_features_with_target_sdf)
    print('assembler done')
    
    reduced_features_with_target_sdf.write.parquet(
    "gs://bigdatamalpredv2/EDAed_data_v2.parquet",
    mode="overwrite")

    spark5.stop()
    print('############## SAVED DATA IN THE BACKEND ######################')