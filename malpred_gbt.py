from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import GBTClassifier

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator

import time

if __name__ == "__main__":
    spark5 = SparkSession.builder \
    .appName("Malware Prediction LR") \
    .config("spark.executor.memory", '12g') \
    .config("spark.driver.maxResultSize", '12g') \
    .config("spark.kryoserializer.buffer.max", '512m') \
    .getOrCreate()    


    # Read the Parquet file into a DataFrame
    reduced_features_with_target_sdf = spark5.read.parquet("gs://dhrithi-yarn-cluster-bucket/EDAed_data.parquet")
    
    # List of subsample percentages
    subsamples = [1.0, 0.8, 0.6, 0.4, 0.2]
    target_column="HasDetections"

    # Initialize evaluators

    binary_evaluator = BinaryClassificationEvaluator(labelCol=target_column)
    regression_evaluator = RegressionEvaluator(labelCol=target_column)


    # Loop over each subsample
    for subsample in subsamples:
        print(f"Training with {subsample*100}% data...")

        start_time = time.time()  # Start the timer

        # Sample data
        # False indicates that we are not sampling with replacement
        sample_data = reduced_features_with_target_sdf.sample(False, subsample, seed=42)

        # Split the data into training set (80%) and test set (20%)
        train, test = sample_data.randomSplit([0.8, 0.2], seed=42)


        # Train a Gradient-Boosted Tree (GBT) model
        gbt = GBTClassifier(featuresCol="features", labelCol="HasDetections", maxDepth=5, maxIter=20)
    
  
        # Define the parameter grid for hyperparameter tuning
        paramGrid = (ParamGridBuilder()
             .addGrid(gbt.maxDepth, [5, 10])
             .addGrid(gbt.maxIter, [20, 30])
             .addGrid(gbt.maxBins, [32, 64 ])
             .build())


        # Create a CrossValidator
        crossval = CrossValidator(estimator=gbt,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(labelCol=target_column),
                          numFolds=3)  # Number of cross-validation folds

        # Run cross-validation and choose the best model
        cvModel = crossval.fit(train)
    
 
        # Make predictions on the testing data using the best model
        predictions = cvModel.transform(test)

        #Evaluate the model's performance
        rmse = regression_evaluator.evaluate(predictions, {regression_evaluator.metricName: "rmse"})

        auc = binary_evaluator.evaluate(predictions)

        end_time = time.time()
 
        # Get the number of cores used
        spark_context = spark5.sparkContext
        num_cores = spark_context.defaultParallelism
        app_id = spark_context.applicationId


        print(f"Subsample: {subsample*100}%")
        print(f"AUC: {auc}")
        print(f"RMSE: {rmse}")

        print(f"Time taken: {end_time - start_time} seconds\n")
        print(f"Number of cores used: {num_cores}")    
        print(f"Spark application ID: {app_id}")





 