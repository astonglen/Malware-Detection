from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import time

if __name__ == "__main__":
    spark5 = SparkSession.builder \
    .appName("Malware Prediction LR") \
    .config("spark.executor.memory", '45g') \
    .config("spark.driver.maxResultSize", '45g') \
    .config("spark.kryoserializer.buffer.max", '512m') \
    .getOrCreate()    
   # .config("spark.default.parallelism", '6') \
    # Read the Parquet file into a DataFrame
    reduced_features_with_target_sdf = spark5.read.parquet("gs://bigdatamalpredv2/EDAed_data_v2.parquet")


    # List of subsample percentages
    subsamples = [1.0]
    target_column="HasDetections"
    # Initialize evaluators
    # BinaryClassificationEvaluator for computing Area Under ROC Curve (AUC)
    binary_evaluator = BinaryClassificationEvaluator(labelCol=target_column)

    # RegressionEvaluator for computing Root Mean Square Error (RMSE)
    regression_evaluator = RegressionEvaluator(labelCol=target_column, metricName="rmse")
    f1_evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName="f1")
    precision_evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName="weightedPrecision")
    recall_evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName="weightedRecall")
    accuracy_evaluator = MulticlassClassificationEvaluator(labelCol=target_column, metricName="accuracy")


    # Loop over each subsample
    for subsample in subsamples:
        print(f"Training with {subsample*100}% data...")

        start_time = time.time()  # Start the timer

        # Sample data
        # False indicates that we are not sampling with replacement
        sample_data = reduced_features_with_target_sdf.sample(False, subsample, seed=42)

        # Split the data into training set (80%) and test set (20%)
        train_data, test_data = sample_data.randomSplit([0.8, 0.2], seed=42)

        # Define the logistic regression model
        lr = LogisticRegression(featuresCol='features', labelCol=target_column)

        # Set up the parameter grid
        # The cross-validator will try all combinations of these parameters
        paramGrid = ParamGridBuilder() \
            .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \
            .addGrid(lr.fitIntercept, [False, True]) \
            .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
            .addGrid(lr.maxIter, [10, 100, 1000]) \
            .build()

        # Set up the cross-validator
        # estimator is the machine learning algorithm to be used (logistic regression in this case)
        # estimatorParamMaps is the grid of parameters to try
        # evaluator is the metric used to measure the performance of the models
        # numFolds is the number of cross-validation folds
        crossval = CrossValidator(estimator=lr,
                                estimatorParamMaps=paramGrid,
                                evaluator=BinaryClassificationEvaluator(labelCol=target_column),
                                numFolds=3)

        # Train the model
        cv_model = crossval.fit(train_data)

        # Make predictions on the test set
        predictions = cv_model.transform(test_data)

        # Evaluate the model using AUC and RMSE
        auc = binary_evaluator.evaluate(predictions)
        rmse = regression_evaluator.evaluate(predictions)

        end_time = time.time()  # Stop the timer
        # Get the number of cores used
        spark_context = spark5.sparkContext
        
        # Get the Spark application ID
        app_id = spark_context.applicationId
        # Compute additional metrics
        f1 = f1_evaluator.evaluate(predictions)
        precision = precision_evaluator.evaluate(predictions)
        recall = recall_evaluator.evaluate(predictions)
        accuracy = accuracy_evaluator.evaluate(predictions)

        
        print(f"Subsample: {subsample*100}%")
        print(f"AUC: {auc}")
        print(f"RMSE: {rmse}")
        print(f"F1 Score: {f1}")
        print(f"Precision: {precision}")
        print(f"Recall: {recall}")
        print(f"Balanced Accuracy: {accuracy}")
        print(f"Time taken: {end_time - start_time} seconds\n")
        print(f"Number of cores used: 6")    
        print(f"Spark application ID: {app_id}")